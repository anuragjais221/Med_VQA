{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import easydict\n",
    "import random\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/anurag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output_chd = path+'/Med_VQA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(path+'/Med_VQA/answer_classes.json', 'r') as j:\n",
    "        answer_classes = json.load(j)\n",
    "\n",
    "\n",
    "l = len(answer_classes) \n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = easydict.EasyDict({\n",
    "        \"SEED\":97,\n",
    "        \"BATCH_SIZE\": 32,\n",
    "        \"VAL_BATCH_SIZE\": 32,\n",
    "        \"NUM_OUTPUT_UNITS\": l,\n",
    "        \"MAX_QUESTION_LEN\": 17,\n",
    "        \"IMAGE_CHANNEL\": 1472,\n",
    "        \"INIT_LERARNING_RATE\": 1e-4,\n",
    "        \"LAMNDA\":0.0001,\n",
    "        \"MFB_FACTOR_NUM\":5,\n",
    "        \"MFB_OUT_DIM\":1000,\n",
    "        \"BERT_UNIT_NUM\":768,\n",
    "        \"BERT_DROPOUT_RATIO\":0.3,\n",
    "        \"MFB_DROPOUT_RATIO\":0.1,\n",
    "        \"NUM_IMG_GLIMPSE\":2,\n",
    "        \"NUM_QUESTION_GLIMPSE\":2,\n",
    "        \"IMG_FEAT_SIZE\":1,\n",
    "        \"IMG_INPUT_SIZE\":224,\n",
    "        \"NUM_EPOCHS\":100,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"torch.backends.cudnn.deterministic=True only applies to CUDA convolution operations, and nothing else. \\nTherefore, no, it will not guarantee that your training process is deterministic, since you're also using\\ntorch.nn.MaxPool3d, whose backward function is nondeterministic for CUDA.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_change = path\n",
    "seed_value = opt.SEED\n",
    "print(seed_value) # 97\n",
    "np.random.seed(seed_value) # return None\n",
    "random.seed(seed_value) # return None\n",
    "torch.manual_seed(seed_value) # return <torch._C.Generator object at 0x7f71cdf7a3d0>\n",
    "torch.cuda.manual_seed(seed_value) # return None\n",
    "torch.cuda.manual_seed_all(seed_value) # return None\n",
    "torch.backends.cudnn.enabled = False \n",
    "''' backends.cudnn.enabled enables cudnn for some operations such as conv layers and RNNs, which can yield \n",
    "a significant speedup. The cudnn RNN implementation doesnâ€™t support the backward operation during eval() \n",
    "and thus raises the error. You could disable cudnn for your workload (as already done) or try to call .train()\n",
    "on the RNN module separately after using model.eval().'''\n",
    "torch.backends.cudnn.benchmark = False\n",
    "'''If your model does not change and your input sizes remain the same - then you may benefit from setting \n",
    "torch.backends.cudnn.benchmark = True.However, if your model changes: for instance, if you have layers that\n",
    "are only \"activated\" when certain conditions are met, or you have layers inside a loop that can be iterated a \n",
    "different number of times, then setting torch.backends.cudnn.benchmark = True might stall your execution.'''\n",
    "torch.backends.cudnn.deterministic = False\n",
    "'''torch.backends.cudnn.deterministic=True only applies to CUDA convolution operations, and nothing else. \n",
    "Therefore, no, it will not guarantee that your training process is deterministic, since you're also using\n",
    "torch.nn.MaxPool3d, whose backward function is nondeterministic for CUDA.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract image feature\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "             We remove all the fully-connected layers in the VGG19 network and the convolution outputs of different feature scales\n",
    "                are concatenated after global average pooling and l2-norm to form a 1984-dimensional vector to represent the image\n",
    "        '''\n",
    "        super(VGG19,self).__init__()\n",
    "        vgg_model = torchvision.models.vgg19(pretrained=True)\t\n",
    "        # resnet_model = torchvision.models.resnet(pretrained=True)\n",
    "\n",
    "        self.Conv1 = nn.Sequential(*list(vgg_model.features.children())[0:4])\n",
    "        self.Conv2 = nn.Sequential(*list(vgg_model.features.children())[4:9])\n",
    "        self.Conv3 = nn.Sequential(*list(vgg_model.features.children())[9:16])\n",
    "        self.Conv4 = nn.Sequential(*list(vgg_model.features.children())[16:23])\n",
    "        self.Conv5 = nn.Sequential(*list(vgg_model.features.children())[23:30])\n",
    "        self.Conv6 = nn.Sequential(*list(vgg_model.features.children())[30:36])\n",
    "\n",
    "        self.avgpool = nn.Sequential(list(vgg_model.children())[1])\n",
    "\n",
    "        # self.inc = nn.Linear(1984,2048)\n",
    "    \n",
    "    def forward(self,image):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out1 = self.Conv1(image)\n",
    "            out2 = self.Conv2(out1)\n",
    "            out3 = self.Conv3(out2)\n",
    "            out4 = self.Conv4(out3)          \n",
    "            out5 = self.Conv5(out4)          # [N, 512, 14, 14]\n",
    "            out6 = self.Conv6(out5) \n",
    "            out7 = self.avgpool(out6)\n",
    "\n",
    "        #global average pooling\n",
    "        out1 = out1.mean([2,3],keepdim=True)\n",
    "        out2 = out2.mean([2,3],keepdim=True)\n",
    "        out3 = out3.mean([2,3],keepdim=True)\n",
    "        out4 = out4.mean([2,3],keepdim=True)\n",
    "        out5 = out5.mean([2,3],keepdim=True)\n",
    "        out6 = out6.mean([2,3],keepdim=True)\n",
    "        out7 = out7.mean([2,3],keepdim=True)\n",
    "\n",
    "       \n",
    "         \n",
    "        concat_features = torch.cat([out1,out2, out3, out4,out5,out6 ], 1) \n",
    "        # print('concat_features',concat_features.shape) # 1,1984,1,1\n",
    "        # with torch.no_grad():\n",
    "        #     concat_features = self.inc(torch.squeeze(concat_features))\n",
    "        \n",
    "        # concat_features = torch.unsqueeze(concat_features, 0)\n",
    "        # concat_features = torch.unsqueeze(concat_features, 2)\n",
    "        # concat_features = torch.unsqueeze(concat_features, 3)\n",
    "\n",
    "        #l2-normalized feature vector\n",
    "        l2_norm = concat_features.norm(p=2, dim=1, keepdim=True).detach() \n",
    "        concat_features = concat_features.div(l2_norm)               # l2-normalized feature vector\n",
    "       \n",
    "\n",
    "        batch_size = concat_features.shape[0]\n",
    "        embedding_dim_size = concat_features.shape[1]\n",
    "        image_feature = concat_features.view(batch_size, embedding_dim_size, -1).squeeze(0) # [N, 1984, 1]\n",
    "\n",
    "        # print('image_feature shape',image_feature.shape)\n",
    "\n",
    "        return image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19_model = VGG19().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vqa = np.load('/home/anurag/Med_VQA/train_dataset_pickle/train19_subset20_21_df.pkl', allow_pickle=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14216, 5)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = opt.IMG_INPUT_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomResizedCrop(size), \n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                    (0.229, 0.224, 0.225))])\n",
    "\n",
    "valid_transform = transforms.Compose([transforms.RandomResizedCrop(size), \n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                    (0.229, 0.224, 0.225))])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop(size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                    (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_feat={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in train_vqa.itertuples():\n",
    "    # print(row)\n",
    "    image = Image.open(row.PATH).convert('RGB')\n",
    "    image = train_transform(image)\n",
    "    # print(type(image))\n",
    "    image_feature = vgg19_model(image[None,...].to(device))\n",
    "    train_image_feat[row.ID] = image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_output_chd+'/train_dataset_pickle/train-image-feature-19-subset20.pickle', 'wb') as f:\n",
    "\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(train_image_feat, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_vqa = np.load('/home/anurag/Med_VQA/valid_dataset_pickle/val19_df.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_vqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_feat={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in val_vqa.itertuples():\n",
    "    # print(row)\n",
    "    image = Image.open(row.PATH).convert('RGB')\n",
    "    image = train_transform(image)\n",
    "    # print(type(image))\n",
    "    image_feature = vgg19_model(image[None,...].to(device))\n",
    "    valid_image_feat[row.ID] = image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_output_chd+'/valid_dataset_pickle/valid-image-feature-19-subset20.pickle', 'wb') as f:\n",
    "\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(valid_image_feat, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vqa = np.load('/home/anurag/Med_VQA/test_dataset_pickle/test19_df.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vqa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_feat={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in test_vqa.itertuples():\n",
    "    # print(row)\n",
    "    image = Image.open(row.PATH).convert('RGB')\n",
    "    image = test_transform(image)\n",
    "    # print(type(image))\n",
    "    image_feature = vgg19_model(image[None,...].to(device))\n",
    "    test_image_feat[row.ID] = image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_output_chd+'/test_dataset_pickle/test-image-feature-19.pickle', 'wb') as f:\n",
    "\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(test_image_feat, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
