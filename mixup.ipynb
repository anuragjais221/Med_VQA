{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/anurag'\n",
    "#****************************To be Change to reproduce ou result*********************************************\n",
    "\n",
    "# location of the data and where to store iamge feature image\n",
    "path_output_chd = path+'/Med_VQA'    \n",
    "\n",
    "input_vqa_train = 'train_dataset_pickle/train19_subset20_21_df.pkl'\n",
    "input_vqa_valid ='valid_dataset_pickle/val19_df.pkl'\n",
    "\n",
    "img_feat_train = 'train_dataset_pickle/train-image-feature-19.pickle'\n",
    "img_feat_valid ='valid_dataset_pickle/valid-image-feature-19.pickle'\n",
    "\n",
    "input_test = 'test_dataset_pickle/test19_df.pkl'\n",
    "img_feat_test = 'test_dataset_pickle/test-image-feature-19.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(path+'/Med_VQA/answer_classes.json', 'r') as j:\n",
    "        answer_classes = json.load(j)\n",
    "\n",
    "\n",
    "l = len(answer_classes) \n",
    "\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "opt = easydict.EasyDict({\n",
    "        \"SEED\":97,\n",
    "        \"BATCH_SIZE\": 64,\n",
    "        \"VAL_BATCH_SIZE\": 64,\n",
    "        \"NUM_OUTPUT_UNITS\": l,\n",
    "        \"MAX_QUESTION_LEN\": 17,\n",
    "        \"IMAGE_CHANNEL\": 1472,\n",
    "        \"INIT_LERARNING_RATE\": 1e-4,\n",
    "        \"LAMNDA\":0.0001,\n",
    "        \"MFB_FACTOR_NUM\":5,\n",
    "        \"MFB_OUT_DIM\":1000,\n",
    "        \"BERT_UNIT_NUM\":768,\n",
    "        \"BERT_DROPOUT_RATIO\":0.3,\n",
    "        \"MFB_DROPOUT_RATIO\":0.1,\n",
    "        \"NUM_IMG_GLIMPSE\":2,\n",
    "        \"NUM_QUESTION_GLIMPSE\":2,\n",
    "        \"IMG_FEAT_SIZE\":1,\n",
    "        \"IMG_INPUT_SIZE\":224,\n",
    "        \"NUM_EPOCHS\":100,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn.functional as f\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"torch.backends.cudnn.deterministic=True only applies to CUDA convolution operations, and nothing else. \\nTherefore, no, it will not guarantee that your training process is deterministic, since you're also using\\ntorch.nn.MaxPool3d, whose backward function is nondeterministic for CUDA.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_change = path\n",
    "seed_value = opt.SEED\n",
    "print(seed_value) # 97\n",
    "np.random.seed(seed_value) # return None\n",
    "random.seed(seed_value) # return None\n",
    "torch.manual_seed(seed_value) # return <torch._C.Generator object at 0x7f71cdf7a3d0>\n",
    "torch.cuda.manual_seed(seed_value) # return None\n",
    "torch.cuda.manual_seed_all(seed_value) # return None\n",
    "torch.backends.cudnn.enabled = False \n",
    "''' backends.cudnn.enabled enables cudnn for some operations such as conv layers and RNNs, which can yield \n",
    "a significant speedup. The cudnn RNN implementation doesn’t support the backward operation during eval() \n",
    "and thus raises the error. You could disable cudnn for your workload (as already done) or try to call .train()\n",
    "on the RNN module separately after using model.eval().'''\n",
    "torch.backends.cudnn.benchmark = False\n",
    "'''If your model does not change and your input sizes remain the same - then you may benefit from setting \n",
    "torch.backends.cudnn.benchmark = True.However, if your model changes: for instance, if you have layers that\n",
    "are only \"activated\" when certain conditions are met, or you have layers inside a loop that can be iterated a \n",
    "different number of times, then setting torch.backends.cudnn.benchmark = True might stall your execution.'''\n",
    "torch.backends.cudnn.deterministic = False\n",
    "'''torch.backends.cudnn.deterministic=True only applies to CUDA convolution operations, and nothing else. \n",
    "Therefore, no, it will not guarantee that your training process is deterministic, since you're also using\n",
    "torch.nn.MaxPool3d, whose backward function is nondeterministic for CUDA.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set path for the dataset\n",
    "train_path = path_change+'/VQA_Med_2019_Dataset/Train/ImageClef-2019-VQA-Med-Training/'#QAPairsByCategory/'\n",
    "train_images_path = path_change+'/VQA_Med_2019_Dataset/Train/ImageClef-2019-VQA-Med-Training/Train_images/'\n",
    "\n",
    "valid_path = path_change+'/VQA_Med_2019_Dataset/Valid/ImageClef-2019-VQA-Med-Validation/'#QAPairsByCategory/'\n",
    "valid_images_path = path_change+'/VQA_Med_2019_Dataset/Valid/ImageClef-2019-VQA-Med-Validation/Val_images/'\n",
    "\n",
    "test_path = path_change+'/VQA_Med_2019_Dataset/Test/VQAMed2019Test/' \n",
    "test_images_path = path_change+'/VQA_Med_2019_Dataset/Test/VQAMed2019Test/VQAMed2019_Test_Images/'\n",
    "\n",
    "size = opt.IMG_INPUT_SIZE\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract image feature\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "             We remove all the fully-connected layers in the VGG19 network and the convolution outputs of different feature scales\n",
    "                are concatenated after global average pooling and l2-norm to form a 1984-dimensional vector to represent the image\n",
    "        '''\n",
    "        super(VGG19,self).__init__()\n",
    "        vgg_model = torchvision.models.vgg19(pretrained=True)\t\n",
    "        # resnet_model = torchvision.models.resnet(pretrained=True)\n",
    "\n",
    "        self.Conv1 = nn.Sequential(*list(vgg_model.features.children())[0:4])\n",
    "        self.Conv2 = nn.Sequential(*list(vgg_model.features.children())[4:9])\n",
    "        self.Conv3 = nn.Sequential(*list(vgg_model.features.children())[9:16])\n",
    "        self.Conv4 = nn.Sequential(*list(vgg_model.features.children())[16:23])\n",
    "        self.Conv5 = nn.Sequential(*list(vgg_model.features.children())[23:30])\n",
    "        self.Conv6 = nn.Sequential(*list(vgg_model.features.children())[30:36])\n",
    "\n",
    "        self.avgpool = nn.Sequential(list(vgg_model.children())[1])\n",
    "\n",
    "        # self.inc = nn.Linear(1984,2048)\n",
    "    \n",
    "    def forward(self,image):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out1 = self.Conv1(image)\n",
    "            # print('out1 shape',out1.shape)\n",
    "            out2 = self.Conv2(out1)\n",
    "            out3 = self.Conv3(out2)\n",
    "            out4 = self.Conv4(out3)          \n",
    "            out5 = self.Conv5(out4)          # [N, 512, 14, 14]\n",
    "            out6 = self.Conv6(out5) \n",
    "            out7 = self.avgpool(out6)\n",
    "\n",
    "        #global average pooling\n",
    "        out1 = out1.mean([2,3],keepdim=True)\n",
    "        out2 = out2.mean([2,3],keepdim=True)\n",
    "        out3 = out3.mean([2,3],keepdim=True)\n",
    "        out4 = out4.mean([2,3],keepdim=True)\n",
    "        out5 = out5.mean([2,3],keepdim=True)\n",
    "        out6 = out6.mean([2,3],keepdim=True)\n",
    "        out7 = out7.mean([2,3],keepdim=True)\n",
    "\n",
    "       \n",
    "         \n",
    "        concat_features = torch.cat([out1,out2, out3, out4,out5,out6 ], 1) \n",
    "        # print('concat_features',concat_features.shape) # 1,1984,1,1\n",
    "        # with torch.no_grad():\n",
    "        #     concat_features = self.inc(torch.squeeze(concat_features))\n",
    "        \n",
    "        # concat_features = torch.unsqueeze(concat_features, 0)\n",
    "        # concat_features = torch.unsqueeze(concat_features, 2)\n",
    "        # concat_features = torch.unsqueeze(concat_features, 3)\n",
    "\n",
    "        #l2-normalized feature vector\n",
    "        l2_norm = concat_features.norm(p=2, dim=1, keepdim=True).detach() \n",
    "        concat_features = concat_features.div(l2_norm)               # l2-normalized feature vector\n",
    "       \n",
    "\n",
    "        batch_size = concat_features.shape[0]\n",
    "        embedding_dim_size = concat_features.shape[1]\n",
    "        image_feature = concat_features.view(batch_size, embedding_dim_size, -1).squeeze(0) # [N, 1984, 1]\n",
    "\n",
    "        # print('image_feature shape',image_feature.shape)\n",
    "\n",
    "        return image_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "vgg19_model = VGG19().to(device)\n",
    "# vgg19_model = ResNet50().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /home/anurag/anaconda3/envs/med_vqa/lib/python3.11/site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "VGG19                                    --\n",
      "├─Sequential: 1-1                        --\n",
      "│    └─Conv2d: 2-1                       1,792\n",
      "│    └─ReLU: 2-2                         --\n",
      "│    └─Conv2d: 2-3                       36,928\n",
      "│    └─ReLU: 2-4                         --\n",
      "├─Sequential: 1-2                        --\n",
      "│    └─MaxPool2d: 2-5                    --\n",
      "│    └─Conv2d: 2-6                       73,856\n",
      "│    └─ReLU: 2-7                         --\n",
      "│    └─Conv2d: 2-8                       147,584\n",
      "│    └─ReLU: 2-9                         --\n",
      "├─Sequential: 1-3                        --\n",
      "│    └─MaxPool2d: 2-10                   --\n",
      "│    └─Conv2d: 2-11                      295,168\n",
      "│    └─ReLU: 2-12                        --\n",
      "│    └─Conv2d: 2-13                      590,080\n",
      "│    └─ReLU: 2-14                        --\n",
      "│    └─Conv2d: 2-15                      590,080\n",
      "│    └─ReLU: 2-16                        --\n",
      "├─Sequential: 1-4                        --\n",
      "│    └─Conv2d: 2-17                      590,080\n",
      "│    └─ReLU: 2-18                        --\n",
      "│    └─MaxPool2d: 2-19                   --\n",
      "│    └─Conv2d: 2-20                      1,180,160\n",
      "│    └─ReLU: 2-21                        --\n",
      "│    └─Conv2d: 2-22                      2,359,808\n",
      "│    └─ReLU: 2-23                        --\n",
      "├─Sequential: 1-5                        --\n",
      "│    └─Conv2d: 2-24                      2,359,808\n",
      "│    └─ReLU: 2-25                        --\n",
      "│    └─Conv2d: 2-26                      2,359,808\n",
      "│    └─ReLU: 2-27                        --\n",
      "│    └─MaxPool2d: 2-28                   --\n",
      "│    └─Conv2d: 2-29                      2,359,808\n",
      "│    └─ReLU: 2-30                        --\n",
      "├─Sequential: 1-6                        --\n",
      "│    └─Conv2d: 2-31                      2,359,808\n",
      "│    └─ReLU: 2-32                        --\n",
      "│    └─Conv2d: 2-33                      2,359,808\n",
      "│    └─ReLU: 2-34                        --\n",
      "│    └─Conv2d: 2-35                      2,359,808\n",
      "│    └─ReLU: 2-36                        --\n",
      "├─Sequential: 1-7                        --\n",
      "│    └─AdaptiveAvgPool2d: 2-37           --\n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(summary(vgg19_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    phase: transforms.Compose([transforms.RandomResizedCrop(size), \n",
    "                                transforms.RandomHorizontalFlip(),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                    (0.229, 0.224, 0.225))]) \n",
    "\n",
    "    for phase in ['train', 'valid']}\n",
    "\n",
    "test_transform = transforms.Compose([transforms.RandomResizedCrop(size),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                    (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(s):\n",
    "    '''remove some character on the question'''\n",
    "    s = s.replace(\" - \", \"-\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(\"\\s\\s+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_df = np.load('/home/anurag/Med_VQA/train_dataset_pickle/train19_subset20_21_df.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>PATH</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synpic41148</td>\n",
       "      <td>what kind of image is this?</td>\n",
       "      <td>cta - ct angiography</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/train_...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synpic43984</td>\n",
       "      <td>is this a t1 weighted image?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/train_...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synpic38930</td>\n",
       "      <td>what type of imaging modality is used to acqui...</td>\n",
       "      <td>us - ultrasound</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/train_...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synpic52143</td>\n",
       "      <td>is this a noncontrast mri?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/train_...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synpic20934</td>\n",
       "      <td>what type of image modality is this?</td>\n",
       "      <td>xr - plain film</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/train_...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                                  Q   \n",
       "0  synpic41148                        what kind of image is this?  \\\n",
       "1  synpic43984                       is this a t1 weighted image?   \n",
       "2  synpic38930  what type of imaging modality is used to acqui...   \n",
       "3  synpic52143                         is this a noncontrast mri?   \n",
       "4  synpic20934               what type of image modality is this?   \n",
       "\n",
       "                      A                                               PATH   \n",
       "0  cta - ct angiography  /home/anurag/Med_VQA/data/raw/ImageCLEF/train_...  \\\n",
       "1                    no  /home/anurag/Med_VQA/data/raw/ImageCLEF/train_...   \n",
       "2       us - ultrasound  /home/anurag/Med_VQA/data/raw/ImageCLEF/train_...   \n",
       "3                    no  /home/anurag/Med_VQA/data/raw/ImageCLEF/train_...   \n",
       "4       xr - plain film  /home/anurag/Med_VQA/data/raw/ImageCLEF/train_...   \n",
       "\n",
       "   labels  \n",
       "0      32  \n",
       "1       2  \n",
       "2      15  \n",
       "3       2  \n",
       "4       5  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feat_train={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in train_dataset_df.itertuples():\n",
    "    # print(row)\n",
    "    image = Image.open(row.PATH).convert('RGB')\n",
    "    image = transform['train'](image)\n",
    "    # print(type(image))\n",
    "    image_feature = vgg19_model(image[None,...].to(device))\n",
    "    image_feat_train[row.ID] = image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_df = np.load('/home/anurag/Med_VQA/valid_dataset_pickle/val19_df.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_image_feat={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in valid_dataset_df.itertuples():\n",
    "    # print(row)\n",
    "    image = Image.open(row.PATH).convert('RGB')\n",
    "    image = transform['valid'](image)\n",
    "    # print(type(image))\n",
    "    image_feature = vgg19_model(image[None,...].to(device))\n",
    "    valid_image_feat[row.ID] = image_feature.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VqaDataset(data.Dataset):\n",
    "    '''\n",
    "        Main class use to retrieve our dataset from pickle file.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_vqa, img_feat_vqa, transform=None, phase = 'train'):\n",
    "        # self.input_dir = input_dir\n",
    "        # self.vqa = np.load(input_dir+'/'+input_vqa, allow_pickle=True )\n",
    "        # self.img_feat_vqa = np.load(input_dir+'/'+img_feat_vqa, allow_pickle=True )\n",
    "        self.vqa = input_vqa\n",
    "        self.vocab_size = None\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "   \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        vqa = self.vqa\n",
    "        phase = self.phase\n",
    "        transform=self.transform[phase]\n",
    "        # img_feat_vqa = self.img_feat_vqa\n",
    "        \n",
    "        # image_id = vqa['image_id'].values[idx]\n",
    "        image_id = vqa['ID'].values[idx]\n",
    "        # image_path = vqa['image_path'].values[idx]\n",
    "        image_path = vqa['PATH'].values[idx]\n",
    "        # image_feat = torch.Tensor(img_feat_vqa[image_id])\n",
    "        # print('image_path',image_path)\n",
    "        \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        # print('transform',transform)\n",
    "        if transform:\n",
    "            image = transform(image)\n",
    "\n",
    "        qst2idc =  vqa['Q'].values[idx]\n",
    "        sample = { 'question': qst2idc, 'image': image, 'image_id':image_id, 'image_path': image_path } \n",
    "        if (self.phase == 'train') or  (self.phase == 'valid'):\n",
    "            ans2idc = vqa['labels'].values[idx]\n",
    "            answer_text = vqa['A'].values[idx]\n",
    "            sample['image_id'] = image_id\n",
    "            sample['label'] = ans2idc\n",
    "            sample['answer_text'] = answer_text\n",
    "        else:\n",
    "            sample['image_id'] = image_id\n",
    "            \n",
    "            \n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.vqa)\n",
    "\n",
    "\n",
    "def get_loader(input_vqa_train, input_vqa_valid, batch_size, num_workers, transform=None,size=228):\n",
    "    '''\n",
    "        Load our dataset with dataloader for the train and valid data\n",
    "    '''\n",
    "\n",
    "    vqa_dataset = {\n",
    "        'train': VqaDataset(\n",
    "            # input_dir=input_dir,\n",
    "            input_vqa=input_vqa_train,\n",
    "            img_feat_vqa=img_feat_train,\n",
    "            transform = transform,\n",
    "            phase = 'train'),\n",
    "        'valid': VqaDataset(\n",
    "            # input_dir=input_dir,\n",
    "            input_vqa=input_vqa_valid,\n",
    "            img_feat_vqa=img_feat_valid,\n",
    "            transform = transform,\n",
    "            phase = 'valid')}\n",
    "    \n",
    "\n",
    "    data_loader = {\n",
    "        phase: torch.utils.data.DataLoader(\n",
    "            dataset=vqa_dataset[phase],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            )\n",
    "        for phase in ['train','valid']}\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def get_test_loader(input_test, batch_size, num_workers,size=228):\n",
    "\n",
    "   \n",
    "    \n",
    "    test_vqa_dataset = VqaDataset(\n",
    "            # input_dir=input_dir,\n",
    "            input_vqa=input_test,\n",
    "            # img_feat_vqa=img_feat_vqa,\n",
    "            phase = 'test')\n",
    "    \n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset=test_vqa_dataset,\n",
    "                                                transform=transform,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=num_workers)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vqa_train = train_dataset_df\n",
    "input_vqa_valid = valid_dataset_df\n",
    "\n",
    "saved_dir = path_output_chd\n",
    "\n",
    "num_epochs = opt.NUM_EPOCHS\n",
    "image_size = opt.IMG_INPUT_SIZE\n",
    "num_workers = 0\n",
    "batch_size = opt.BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = get_loader(\n",
    "        input_vqa_train = train_dataset_df, \n",
    "        input_vqa_valid = input_vqa_valid,\n",
    "        batch_size = batch_size, \n",
    "        num_workers = num_workers,\n",
    "        transform = transform,\n",
    "        size = image_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data_vqa(id,v,p, q, a, al, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed images, pairs of questions, targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = v.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    # print('index',index)\n",
    "\n",
    "    mixed_v = lam * v + (1 - lam) * v[index, :]\n",
    "    # print('mixed_v',mixed_v.shape)\n",
    "    id_a, id_b = id,id[index]\n",
    "    p_a,p_b = p,p[index]\n",
    "    a_a, a_b = a, a[index]\n",
    "    al_a,al_b = al,al[index]\n",
    "    # print('a_a',a_a)\n",
    "    # print('a_b',a_b)\n",
    "    q_a, q_b = q, q[index]\n",
    "    # print('q_a',q_a)\n",
    "    # print('q_b',q_b)\n",
    "    \n",
    "    return id_a,id_b,mixed_v,p_a,p_b, a_a, a_b,al_a,al_b, q_a, q_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_feat = {}\n",
    "dict_data={'image_id': [],  'image_path_a': [], 'image_path_b':[] ,'question_a': [], 'question_b':[], 'labels_a':[] , 'labels_b':[], 'answer_a' : [], 'answer_b':[], 'lamda':[]  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, batch_sample in enumerate(data_loader['train']):\n",
    "    image_id = batch_sample['image_id']\n",
    "    image_path = batch_sample['image_path']\n",
    "    image = batch_sample['image'].to(device)\n",
    "    questions = batch_sample['question']\n",
    "    labels = batch_sample['label'].to(device)\n",
    "    label_answer_text = batch_sample['answer_text']\n",
    "    le1 = LabelEncoder()\n",
    "    label_answers_text_l = le1.fit_transform(label_answer_text)\n",
    "    label_answers_text_l = torch.as_tensor(label_answers_text_l).to(device)\n",
    "    le2 = LabelEncoder() \n",
    "    question_labels = le2.fit_transform(questions)\n",
    "    question_labels = torch.as_tensor(question_labels).to(device)\n",
    "    le3 = LabelEncoder()\n",
    "    image_id_labels = le3.fit_transform(image_id)\n",
    "    image_id_labels = torch.as_tensor(image_id_labels).to(device)\n",
    "    le4 = LabelEncoder()\n",
    "    image_path_labels = le4.fit_transform(image_path)\n",
    "    image_path_labels = torch.as_tensor(image_path_labels).to(device)\n",
    "\n",
    "    id_a,id_b,mixed_v, p_a,p_b,a_a, a_b,al_a,al_b, q_a, q_b, lam = mixup_data_vqa(\n",
    "                image_id_labels,image, image_path_labels,question_labels, labels, label_answers_text_l, alpha=0.1, use_cuda=True)\n",
    "\n",
    "    # mixed_v = mixed_v.cpu().data.numpy()\n",
    "    a_a = a_a.cpu().data.numpy()\n",
    "    a_b = a_b.cpu().data.numpy()\n",
    "    id_a = id_a.cpu().data.numpy()\n",
    "    id_b = id_b.cpu().data.numpy()\n",
    "    q_a = q_a.cpu().data.numpy()\n",
    "    q_b = q_b.cpu().data.numpy()\n",
    "    al_a = al_a.cpu().data.numpy() \n",
    "    al_b = al_b.cpu().data.numpy()\n",
    "    p_a = p_a.cpu().data.numpy()\n",
    "    p_b = p_b.cpu().data.numpy()\n",
    "    p_a_l = le4.inverse_transform(p_a)\n",
    "    p_b_l = le4.inverse_transform(p_b)\n",
    "    id_a_l = le3.inverse_transform(id_a)\n",
    "    id_b_l = le3.inverse_transform(id_b)\n",
    "    q_a_l = le2.inverse_transform(q_a)\n",
    "    q_b_l = le2.inverse_transform(q_b)\n",
    "    al_a_l = le1.inverse_transform(al_a)\n",
    "    al_b_l = le1.inverse_transform(al_b)\n",
    "    \n",
    "\n",
    "\n",
    "    for index,img in enumerate(mixed_v):\n",
    "        # print('img_shape',img.shape)\n",
    "        mix_image_feature = vgg19_model(img[None,...]).to(device)\n",
    "        # print('mix_img_feature.shape',mix_image_feature.shape)\n",
    "        image_feat[id_a_l[index]+'_'+id_b_l[index]] = mix_image_feature.cpu().numpy()\n",
    "        image_feat_train[id_a_l[index]+'_'+id_b_l[index]] = mix_image_feature.cpu().numpy()\n",
    "\n",
    "    # imshow(torchvision.utils.make_grid(mixed_v))\n",
    "    mixed_v = mixed_v.cpu().data.numpy()\n",
    "    \n",
    "    for index in range(len(mixed_v)):\n",
    "        # print(id_a[index])\n",
    "        image_name = id_a_l[index]+'_'+id_b_l[index]\n",
    "        dict_data['image_id'].append(image_name)\n",
    "        # dict_data['image_id'].append(id_a_l[index]+'_'+id_b_l[index])\n",
    "        # dict_data['mixed_image'].append(mixed_v[index])\n",
    "        # dict_data['mixed_image'].append(mixed_v[index])\n",
    "        dict_data['image_path_a'].append(p_a_l[index])\n",
    "        dict_data['image_path_b'].append(p_b_l[index])\n",
    "        dict_data['question_a'].append(q_a_l[index])\n",
    "        dict_data['question_b'].append(q_b_l[index])\n",
    "        dict_data['labels_a'].append(a_a[index])\n",
    "        dict_data['labels_b'].append(a_b[index])\n",
    "        dict_data['answer_a'].append(al_a_l[index])\n",
    "        dict_data['answer_b'].append(al_b_l[index])\n",
    "        \n",
    "        dict_data['lamda'].append(lam)\n",
    "        # mixed_image = np.transpose(mixed_v[index], (1, 2, 0))\n",
    "        # print(mixed_image.min())\n",
    "        # mini = mixed_image.min()\n",
    "        # maxi = mixed_image.max()\n",
    "        # print(mixed_image.max())\n",
    "        # print(mini)\n",
    "        # print(maxi)\n",
    "        # plt.imshow(mixed_image)\n",
    "        # new_mixed_image = (1/(mini*(-1)*maxi)) * mixed_image + 0.5\n",
    "        # new_mixed_image = mixed_image/(maxi-mini) + 0.5\n",
    "        # plt.imsave('myy.png',(mixed_image).astype(np.uint8),cmap='gray')\n",
    "        # fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n",
    "        # axs[0].imshow(Image.open(p_a_l[index]).convert('RGB'))\n",
    "        # axs[0].set_title(id_a_l[index])\n",
    "        # axs[1].imshow(Image.open(p_b_l[index]).convert('RGB'))\n",
    "        # axs[1].set_title(id_b_l[index])\n",
    "        # axs[2].imshow(mixed_image)\n",
    "        # axs[2].set_title(image_name)\n",
    "        # # plt.show()\n",
    "        # fig.savefig(path+'/Med_VQA/data/raw/mixed/'+image_name+'.png',bbox_inches='tight',pad_inches=1)\n",
    "\n",
    "        # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dict dict_keys(['ID', 'Q', 'A', 'PATH', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "train_dict = train_dataset_df.to_dict('list')\n",
    "print('train_dict',train_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_data:\n",
    "    if key == 'image_id':\n",
    "        for i in train_dict['ID']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'image_path_a':\n",
    "        for i in train_dict['PATH']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'image_path_b':\n",
    "        for i in train_dict['PATH']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'question_a':\n",
    "        for i in train_dict['Q']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'question_b':\n",
    "        for i in train_dict['Q']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'answer_a':\n",
    "        for i in train_dict['A']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'answer_b':\n",
    "        for i in train_dict['A']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'labels_a':\n",
    "        for i in train_dict['labels']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'labels_b':\n",
    "        for i in train_dict['labels']:\n",
    "            dict_data[key].append(i)\n",
    "    elif key == 'lamda':\n",
    "        for i in train_dict['Q']:\n",
    "            dict_data[key].append(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_train_df_data = pd.DataFrame(dict_data, columns = ['image_id',\n",
    "                                                    'mixed_image', \n",
    "                                                    'question_a',\n",
    "                                                    'question_b',\n",
    "                                                    'answer_a',\n",
    "                                                    'answer_b',\n",
    "                                                    'labels_a',\n",
    "                                                    'labels_b',\n",
    "                                                    'lamda'\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>mixed_image</th>\n",
       "      <th>question_a</th>\n",
       "      <th>question_b</th>\n",
       "      <th>answer_a</th>\n",
       "      <th>answer_b</th>\n",
       "      <th>labels_a</th>\n",
       "      <th>labels_b</th>\n",
       "      <th>lamda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synpic26679_synpic17696</td>\n",
       "      <td>NaN</td>\n",
       "      <td>which organ system is imaged?</td>\n",
       "      <td>what is abnormal in the ct scan?</td>\n",
       "      <td>vascular and lymphatic</td>\n",
       "      <td>sarcoidosis</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>0.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synpic52640_synpic19015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what imaging modality is used to acquire this ...</td>\n",
       "      <td>what imaging method was used?</td>\n",
       "      <td>xr - plain film</td>\n",
       "      <td>xr - plain film</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synpic46480_synpic26947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>is this a ct scan?</td>\n",
       "      <td>which image modality is this?</td>\n",
       "      <td>no</td>\n",
       "      <td>nm - nuclear medicine</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>0.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synpic54051_synpic54829</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what is the primary abnormality in this image?</td>\n",
       "      <td>what organ system is shown in the image?</td>\n",
       "      <td>leptomeningeal sarcoid</td>\n",
       "      <td>musculoskeletal</td>\n",
       "      <td>177</td>\n",
       "      <td>6</td>\n",
       "      <td>0.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synpic48337_synpic41703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what part of the body does this mri show?</td>\n",
       "      <td>what is the primary abnormality in this image?</td>\n",
       "      <td>skull and contents</td>\n",
       "      <td>plantar fasciitis</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>0.185757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28427</th>\n",
       "      <td>synpic44352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what is abnormal in the mri?</td>\n",
       "      <td>what is abnormal in the mri?</td>\n",
       "      <td>neuroblastoma</td>\n",
       "      <td>neuroblastoma</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28428</th>\n",
       "      <td>synpic34315</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what abnormality is seen in the image?</td>\n",
       "      <td>what abnormality is seen in the image?</td>\n",
       "      <td>adenocarcinoma of the lung</td>\n",
       "      <td>adenocarcinoma of the lung</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28429</th>\n",
       "      <td>synpic57872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what is abnormal in the angiogram?</td>\n",
       "      <td>what is abnormal in the angiogram?</td>\n",
       "      <td>dural  fistula, avf</td>\n",
       "      <td>dural  fistula, avf</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28430</th>\n",
       "      <td>synpic17802</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what is abnormal in the ct scan?</td>\n",
       "      <td>what is abnormal in the ct scan?</td>\n",
       "      <td>osteoid osteoma</td>\n",
       "      <td>osteoid osteoma</td>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28431</th>\n",
       "      <td>synpic40381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what is most alarming about this ct scan?</td>\n",
       "      <td>what is most alarming about this ct scan?</td>\n",
       "      <td>adenocarcinoma of the lung</td>\n",
       "      <td>adenocarcinoma of the lung</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28432 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image_id mixed_image   \n",
       "0      synpic26679_synpic17696         NaN  \\\n",
       "1      synpic52640_synpic19015         NaN   \n",
       "2      synpic46480_synpic26947         NaN   \n",
       "3      synpic54051_synpic54829         NaN   \n",
       "4      synpic48337_synpic41703         NaN   \n",
       "...                        ...         ...   \n",
       "28427              synpic44352         NaN   \n",
       "28428              synpic34315         NaN   \n",
       "28429              synpic57872         NaN   \n",
       "28430              synpic17802         NaN   \n",
       "28431              synpic40381         NaN   \n",
       "\n",
       "                                              question_a   \n",
       "0                          which organ system is imaged?  \\\n",
       "1      what imaging modality is used to acquire this ...   \n",
       "2                                     is this a ct scan?   \n",
       "3         what is the primary abnormality in this image?   \n",
       "4              what part of the body does this mri show?   \n",
       "...                                                  ...   \n",
       "28427                       what is abnormal in the mri?   \n",
       "28428             what abnormality is seen in the image?   \n",
       "28429                 what is abnormal in the angiogram?   \n",
       "28430                   what is abnormal in the ct scan?   \n",
       "28431          what is most alarming about this ct scan?   \n",
       "\n",
       "                                           question_b   \n",
       "0                    what is abnormal in the ct scan?  \\\n",
       "1                       what imaging method was used?   \n",
       "2                       which image modality is this?   \n",
       "3            what organ system is shown in the image?   \n",
       "4      what is the primary abnormality in this image?   \n",
       "...                                               ...   \n",
       "28427                    what is abnormal in the mri?   \n",
       "28428          what abnormality is seen in the image?   \n",
       "28429              what is abnormal in the angiogram?   \n",
       "28430                what is abnormal in the ct scan?   \n",
       "28431       what is most alarming about this ct scan?   \n",
       "\n",
       "                         answer_a                    answer_b  labels_a   \n",
       "0          vascular and lymphatic                 sarcoidosis        18  \\\n",
       "1                 xr - plain film             xr - plain film         5   \n",
       "2                              no       nm - nuclear medicine         2   \n",
       "3          leptomeningeal sarcoid             musculoskeletal       177   \n",
       "4              skull and contents           plantar fasciitis         1   \n",
       "...                           ...                         ...       ...   \n",
       "28427               neuroblastoma               neuroblastoma       105   \n",
       "28428  adenocarcinoma of the lung  adenocarcinoma of the lung        71   \n",
       "28429         dural  fistula, avf         dural  fistula, avf       107   \n",
       "28430             osteoid osteoma             osteoid osteoma       113   \n",
       "28431  adenocarcinoma of the lung  adenocarcinoma of the lung        71   \n",
       "\n",
       "       labels_b     lamda  \n",
       "0            66  0.185757  \n",
       "1             5  0.185757  \n",
       "2            47  0.185757  \n",
       "3             6  0.185757  \n",
       "4           177  0.185757  \n",
       "...         ...       ...  \n",
       "28427       105  1.000000  \n",
       "28428        71  1.000000  \n",
       "28429       107  1.000000  \n",
       "28430       113  1.000000  \n",
       "28431        71  1.000000  \n",
       "\n",
       "[28432 rows x 9 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_train_df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28432, 9)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_train_df_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name='mixed_train_dataset_df_19_subset2021'\n",
    "mixed_train_df_data.to_pickle(path_output_chd+'/train_dataset_pickle/'+pickle_name+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_output_chd+'/train_dataset_pickle/mixed-and-original-train-image-feature-19-subset2021.pickle', 'wb') as f:\n",
    "\n",
    "    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "    pickle.dump(image_feat_train, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.load('/home/anurag/Med_VQA/test_dataset_pickle/test19_df.pkl',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Task</th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>PATH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>synpic54082</td>\n",
       "      <td>modality</td>\n",
       "      <td>what modality is shown?</td>\n",
       "      <td>cta - ct angiography</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>synpic48556</td>\n",
       "      <td>modality</td>\n",
       "      <td>was the mri taken with contrast?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>synpic50696</td>\n",
       "      <td>modality</td>\n",
       "      <td>what type of contrast did this patient have?</td>\n",
       "      <td>iv</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>synpic37194</td>\n",
       "      <td>modality</td>\n",
       "      <td>what imaging method was used?</td>\n",
       "      <td>us-d - doppler ultrasound</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synpic31308</td>\n",
       "      <td>modality</td>\n",
       "      <td>what modality is shown?</td>\n",
       "      <td>an - angiogram</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>synpic21789</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>what is abnormal in the ct scan?</td>\n",
       "      <td>ossification of stylohyoid ligament</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>synpic39878</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>what is most alarming about this mri?</td>\n",
       "      <td>cerebral infarct</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>synpic41525</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>what is most alarming about this ct scan?</td>\n",
       "      <td>epiploic appendagitis</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>synpic18173</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>what is the primary abnormality in this image?</td>\n",
       "      <td>pancreatic ductal adenocarcinoma</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>synpic54143</td>\n",
       "      <td>abnormality</td>\n",
       "      <td>what is the primary abnormality in this image?</td>\n",
       "      <td>anaplastic oligodendroglioma#multi-cystic comp...</td>\n",
       "      <td>/home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID         Task                                               Q   \n",
       "0    synpic54082     modality                         what modality is shown?  \\\n",
       "1    synpic48556     modality                was the mri taken with contrast?   \n",
       "2    synpic50696     modality    what type of contrast did this patient have?   \n",
       "3    synpic37194     modality                   what imaging method was used?   \n",
       "4    synpic31308     modality                         what modality is shown?   \n",
       "..           ...          ...                                             ...   \n",
       "495  synpic21789  abnormality                what is abnormal in the ct scan?   \n",
       "496  synpic39878  abnormality           what is most alarming about this mri?   \n",
       "497  synpic41525  abnormality       what is most alarming about this ct scan?   \n",
       "498  synpic18173  abnormality  what is the primary abnormality in this image?   \n",
       "499  synpic54143  abnormality  what is the primary abnormality in this image?   \n",
       "\n",
       "                                                     A   \n",
       "0                                 cta - ct angiography  \\\n",
       "1                                                   no   \n",
       "2                                                   iv   \n",
       "3                            us-d - doppler ultrasound   \n",
       "4                                       an - angiogram   \n",
       "..                                                 ...   \n",
       "495                ossification of stylohyoid ligament   \n",
       "496                                   cerebral infarct   \n",
       "497                              epiploic appendagitis   \n",
       "498                   pancreatic ductal adenocarcinoma   \n",
       "499  anaplastic oligodendroglioma#multi-cystic comp...   \n",
       "\n",
       "                                                  PATH  \n",
       "0    /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "1    /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "2    /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "3    /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "4    /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "..                                                 ...  \n",
       "495  /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "496  /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "497  /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "498  /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "499  /home/anurag/Med_VQA/data/raw/ImageCLEF/test_2...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med_vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
